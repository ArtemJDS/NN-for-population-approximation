{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3hZ5TYi7yTC6mqJ1aAzmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtemJDS/NN-for-population-approximation/blob/main/Koopman_autoencoder_for_population.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nKv0b7b3woF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/voltage_and_input.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = h5py.File('/content/content/voltage_and_input.h5', 'r')\n",
        "voltage = np.array(dataset['voltage'], dtype = np.float32).T    # population's voltage dynamics               \n",
        "input = np.array(dataset['input'], dtype = np.float32)     # population's input   \n",
        "\n",
        "min = np.min(voltage)\n",
        "max = np.max(voltage)\n",
        "bins = np.linspace(min, max, N_BINS)     \n",
        "x = np.apply_along_axis(np.histogram, 1, voltage, bins = bins, density = True).T[0]\n",
        "x = list(x)\n",
        "x = np.stack(x)\n",
        "densities = normalize(x, norm = 'l1') # shape = n,m, where n = # of timesteps, m = N_BINS"
      ],
      "metadata": {
        "id": "yEkafAA76RDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_dataset = np.array([densities[:-5], densities[1:-4], densities[2:-3], densities[3:-2], densities[4:-1]])\n",
        "train_dataset = tf.convert_to_tensor(train_dataset)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_dataset).shuffle(1104636).batch(batch_size)"
      ],
      "metadata": {
        "id": "rCscbcpW69lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is an attempt to apply koopmanism for population's voltage distribution approximation.\n",
        "\n",
        "The procedure in general follows the described in \"Lusch, B., Kutz, J.N. & Brunton, S.L. Deep learning for universal linear embeddings of nonlinear dynamics. Nat Commun 9, 4950 (2018)\"\n",
        "\n",
        "train_dataset contains n (here n equals 5) ordered snapshots of a system. Network's task is approximation of the system's trajectory. \n",
        "\n",
        "It's achieved by means of nonlinear dimensionality reduction (autoencoder). Then system is propagated linearly (Koopman operator) in the new coordinates. At each step approximated distribution gets compared with the actual one. \n"
      ],
      "metadata": {
        "id": "TWeaRFLt_-B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LATENT_DIM = 2\n",
        "N_BINS = 249\n",
        "\n",
        "class Autoencoder(Model):\n",
        "    def __init__(self, latent_dim, n_bins):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.latent_dim = latent_dim   \n",
        "        self.n_bins = n_bins\n",
        "\n",
        "        self.main_encoder = tf.keras.Sequential([\n",
        "                            layers.Flatten(),\n",
        "                            layers.Dense(256, activation='relu', kernel_regularizer='l2'), \n",
        "                            layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
        "                            layers.Dense(self.latent_dim,activation='relu', kernel_regularizer='l2'),\n",
        "                            ])\n",
        "      \n",
        "        self.Koopman = tf.keras.Sequential([\n",
        "                            layers.Dense(self.latent_dim, use_bias=True)\n",
        "                        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "                        layers.Dense(256, activation='relu', kernel_regularizer='l2'),\n",
        "                        layers.Dense(256, activation='relu', kernel_regularizer='l2'), \n",
        "                        layers.Dense(self.n_bins, activation='relu', kernel_regularizer='l2'),\n",
        "                        ])\n",
        "\n",
        "    def call(self, input):\n",
        "\n",
        "        nth_main = tf.zeros([batch_size, 2])\n",
        "        encoded_main = tf.zeros([batch_size, latent_dim])\n",
        "        K_activated = tf.zeros([batch_size, latent_dim])\n",
        "        decoded = tf.zeros([batch_size, 2])\n",
        "        eigenvalues = tf.zeros([batch_size, latent_dim])\n",
        "\n",
        "        n_plus_one_main = nth_main\n",
        "        encoded_n_plus_main = encoded_main\n",
        "\n",
        "        loss_decode = 0.\n",
        "        loss_repr = 0.\n",
        "        loss_der = 0.\n",
        "\n",
        "        input = tf.transpose(input, [1,0,2])\n",
        "        l = input.shape[0] - 1\n",
        "        for n in tf.range(l):\n",
        "\n",
        "            i = input[n]\n",
        "            j = input[n+1]\n",
        "            if n == 0:\n",
        "              tr = tf.transpose(i) \n",
        "             \n",
        "              nth_main = tf.reshape(tf.transpose(tr[:]), [-1,2])\n",
        "              encoded_main = self.main_encoder(nth_main)\n",
        "              K_activated = self.Koopman(encoded_main)\n",
        "              decoded = self.decoder(encoded_main)  \n",
        "\n",
        "              tr_new = tf.transpose(j)\n",
        "              n_plus_one_main = tf.reshape(tf.transpose(tr_new[:]), [-1,2])\n",
        "              encoded_n_plus_main = self.main_encoder(n_plus_one_main)\n",
        "\n",
        "              loss_decode +=  tf.math.reduce_sum(tf.math.square(nth_main - decoded))\n",
        "              loss_repr += tf.math.reduce_sum(tf.math.square(K_activated - encoded_n_plus_main))\n",
        "            \n",
        "            if n != 0:\n",
        "\n",
        "              K_activated = self.Koopman(K_activated)\n",
        "              decoded = self.decoder(K_activated)\n",
        "\n",
        "              tr_new = tf.transpose(j)\n",
        "              n_plus_one_main = tf.reshape(tf.transpose(tr_new[:]), [-1,2])\n",
        "              encoded_n_plus_main = self.main_encoder(n_plus_one_main)\n",
        "\n",
        "              loss_decode += tf.math.reduce_sum(tf.math.square(tf.transpose(tf.transpose(n_plus_one_main)[:]) - decoded))\n",
        "              loss_repr += tf.math.reduce_sum(tf.math.square(K_activated - encoded_n_plus_main))\n",
        "\n",
        "        return loss_repr, loss_decode, loss_decode + loss_repr * 0.0005\n",
        "autoencoder = Autoencoder(LATENT_DIM)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
      ],
      "metadata": {
        "id": "w1JPy4cK8stW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHES = 100\n",
        "\n",
        "@tf.function\n",
        "def training_step(inp):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_a, loss_b, loss = autoencoder(inp)\n",
        "    grads = tape.gradient(loss, autoencoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "    return loss_a, loss_b\n",
        "\n",
        "\n",
        "l = []\n",
        "for n in range(EPOCHES):\n",
        "        l1 = []\n",
        "        l2 = []\n",
        "\n",
        "        for inp in dataset_train: \n",
        "    \n",
        "            loss1, loss2 = training_step(inp)\n",
        "            l1.append(float(loss1))\n",
        "            l2.append(float(loss2))\n",
        "\n",
        "        print(f'ITERATION {n}, {round(sum(l1)/len(l1), 8)}, {round(sum(l2)/len(l2), 8)}')\n",
        "\n",
        "        l.append(l1)\n",
        "        l.append(l2)\n"
      ],
      "metadata": {
        "id": "hNSysgSv_DDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}