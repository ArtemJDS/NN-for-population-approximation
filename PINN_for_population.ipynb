{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYBiqAw+hmerLmoagyqEpz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArtemJDS/NN-for-population-approximation/blob/main/PINN_for_population.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vpSaqnKY8kT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/voltage_and_input.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_BINS = 249\n",
        "INPUT_EPOCH = 2500"
      ],
      "metadata": {
        "id": "WHB4pguGbUTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = h5py.File('/content/content/voltage_and_input.h5', 'r')\n",
        "voltage = np.array(dataset['voltage'], dtype = np.float32).T    # population's voltage dynamics               \n",
        "input = np.array(dataset['input'], dtype = np.float32)     # population's input   \n",
        "\n",
        "min = np.min(voltage)\n",
        "max = np.max(voltage)\n",
        "bins = np.linspace(min, max, N_BINS)     \n",
        "x = np.apply_along_axis(np.histogram, 1, voltage, bins = bins, density = True).T[0]\n",
        "x = list(x)\n",
        "x = np.stack(x)\n",
        "densities = normalize(x, norm = 'l1') # shape = n,m, where n = # of timesteps, m = N_BINS"
      ],
      "metadata": {
        "id": "twd9gK_lZ5ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take one epoch for input to be constant over all timesteps\n",
        "\n",
        "densities_test = densities[INPUT_EPOCH*21:INPUT_EPOCH*22]\n",
        "derivatives_t = (densities_test[1:] - densities_test[:-1])   # time derivative of density for each datapoint but the last timestep\n",
        "\n",
        "xs = np.arange(0,N_BINS)    # quasi-spatial coordinate\n",
        "ts = np.arange(0,1999)    # time coordinate\n",
        "ps = densities_test[:-1]    # density \n",
        "\n",
        "xs = np.tile(xs, (1999,1))\n",
        "ts = np.tile(ts, (N_BINS, 1)).T\n",
        "\n",
        "ts = normalize(ts)\n",
        "xs = normalize(xs)\n",
        "derivatives_t = normalize(derivatives_t)\n",
        "\n",
        "ts = ts.reshape(-1)\n",
        "xs = xs.reshape(-1)\n",
        "ps = ps.reshape(-1)    \n",
        "derivatives_t = derivatives_t.reshape(-1)    # reshape all to one tall vector \n",
        "\n",
        "input = np.vstack((ts, xs)).T#, ps)).T    # input to the network. time coordinate, spatial coordinate, density for each time-place pair \n",
        "\n",
        "DT = ts[1] - ts[0]\n",
        "DX = xs[1] - xs[0]"
      ],
      "metadata": {
        "id": "pacjYVJubkH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = N_BINS    # batch_size MUST be divisible by N_BINS - 1 because of network loss function design\n",
        "input = tf.convert_to_tensor(input)\n",
        "derivatives_t = tf.convert_to_tensor(derivatives_t)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input, derivatives_t)).shuffle(495752).batch(batch_size) "
      ],
      "metadata": {
        "id": "lL4h8RbBdA96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network approximates temporal derivative of population's voltage distribution. \n",
        "\\begin{align}\n",
        "\\ T = \\frac{\\partial P} {\\partial t} \n",
        "\\end{align}\n",
        "Spatial and temporal coordinates are independent, hence\n",
        "\n",
        "\\begin{align}\n",
        "\\ T = \\frac{dP} {dt} \n",
        "\\end{align}\n",
        "\n",
        "Approximating loss is defined as follows:\n",
        "\n",
        "\\begin{align}\n",
        "\\ \\mathcal{L}_{AP} = (T - \\hat{T})^2\n",
        "\\end{align}\n",
        "where $\\hat{T}$ is the approximation\n",
        "\n",
        "Network is presupposed to use some mathematical properties of distributions \n",
        "\\begin{align}\n",
        "\\ \\int_{0}^{X} T \\,dx = 1 \\text{ for each t }\n",
        "\\end{align}\n",
        "because total density must be conserved.\n",
        "\n",
        "So total density loss is defined as\n",
        "\\begin{align}\n",
        "\\ \\mathcal{L}_{DC} = ((\\int_{0}^{X} \\hat{T} \\,dx ) - 1 )^2\n",
        "\\end{align}\n",
        "\n",
        "As $T$ is the temporal derivative of some function defined over 2D  it is a conservative field and its derivative must match $\\frac{dT} {dt} = \\frac{dT} {dx}$ (written as full derivatives).\n",
        "\n",
        "Derivative loss is defined as\n",
        "\\begin{align}\n",
        "\\ \\mathcal{L}_{DR} = (\\frac{d\\hat{T}} {dt} - \\frac{d\\hat{T}} {dx} )^2\n",
        "\\end{align}\n",
        "\n",
        "The previous loss paves the way for derivative zeroing (both $\\frac{dT} {dt}$ and  $\\frac{dP} {dx}$ becoming zero). Therefore we add loss that prevents it.\n",
        "\\begin{align}\n",
        "\\ \\mathcal{L}_{AZ} = \\log \\frac {1} {\\|\\frac{d\\hat{T}} {dt}\\| + \\|\\frac{d\\hat{T}} {dx}\\|}\n",
        "\\end{align}\n",
        "\n",
        "Total loss is a mere sum of previously defined losses with coefficients"
      ],
      "metadata": {
        "id": "bpfeI0PypC9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def training_step(inp):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss, total_density_loss, derivative_loss, anti_zeroing_loss = model(inp)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return loss, total_density_loss, derivative_loss, anti_zeroing_loss\n",
        "\n",
        "\n",
        "class ProbabilityTimeDerivativeApproximator(Model):\n",
        "    def __init__(self, ts, xs):\n",
        "       super(ProbabilityTimeDerivativeApproximator, self).__init__()\n",
        "       self.layer_list = []\n",
        "       self.layer_list.append(tf.keras.Sequential([layers.Flatten(),layers.Dense(128, activation='sigmoid')]))\n",
        "       self.layer_list.append(tf.keras.Sequential([layers.Dense(128, activation='sigmoid')]))\n",
        "       self.layer_list.append(tf.keras.Sequential([layers.Dense(128, activation='sigmoid')]))\n",
        "       self.layer_list.append(tf.keras.Sequential([layers.Dense(128, activation='sigmoid'),\n",
        "                                               layers.Dense(1, activation='sigmoid'),]))\n",
        "       self.ts = ts\n",
        "       self.xs = xs\n",
        "\n",
        "    def call(self, input):\n",
        "        \n",
        "        for_network, real_derivatives  =  input\n",
        "\n",
        "        for n, layer in enumerate(self.layer_list[:-1]):\n",
        "            if n!= 0: x = layer(x)\n",
        "            if n == 0: x = layer(for_network)\n",
        "        approximated_derivatives = self.layer_list[-1](x)\n",
        "        approximation_loss = tf.math.reduce_sum(tf.math.square(real_derivatives - approximated_derivatives))\n",
        "\n",
        "        dIdt_dIdx = tf.transpose(tf.gradients(approximated_derivatives, for_network)[0])[:2]\n",
        "        total_density_loss = tf.math.square(1. - tf.math.reduce_sum(approximated_derivatives))\n",
        "        int_loss_scale = 1e-0\n",
        "        derivative_loss = tf.math.reduce_sum(tf.math.square(dIdt_dIdx[0] * self.ts - dIdt_dIdx[1] * self.xs)) * int_loss_scale\n",
        "        anti_zeroing_loss = tf.math.reduce_sum(tf.math.square(dIdt_dIdx[0])) + tf.math.reduce_sum(tf.math.square(dIdt_dIdx[1]))\n",
        "        loss = approximation_loss + total_density_loss + tf.math.log(1/anti_zeroing_loss) #+ derivative_loss #+ tf.math.log(1/anti_zeroing_loss)\n",
        "\n",
        "        return loss, total_density_loss, derivative_loss, anti_zeroing_loss\n",
        "model = ProbabilityTimeDerivativeApproximator(DT, DX)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "GRESpXNqdcDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHES = 10\n",
        "for k in range(EPOCHES):\n",
        "    l = 0.\n",
        "    i = 0.\n",
        "    d = 0.\n",
        "    az = 0.\n",
        "    for inp in train_dataset: \n",
        "        loss, total_density_loss, derivative_loss, anti_zeroing_loss = training_step(inp)\n",
        "        l += loss\n",
        "        i += total_density_loss\n",
        "        d += derivative_loss\n",
        "        az += anti_zeroing_loss\n",
        "\n",
        "\n",
        "    print(f'EPOCH {k}', float(l), float(i), float(d), float(az))"
      ],
      "metadata": {
        "id": "-uK4SIW8gItF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}